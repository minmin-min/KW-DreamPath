# ============================================
# ğŸ“š SBERT ê¸°ë°˜ ê°•ì˜ ì¶”ì²œ ì‹œìŠ¤í…œ (ê°•ì˜ì •ë³´ ì „ìš© + ê³¼ëª© ë‹¨ìœ„ + index=2 ì œì™¸)
# ============================================

import os
import psycopg2
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# ===== .env ë¶ˆëŸ¬ì˜¤ê¸° =====
load_dotenv()

# ===== ê¸°ë³¸ ì„¤ì • =====
PG_DSN = {
    "host": "localhost",
    "dbname": "kwchatbot",
    "user": "postgres",
    "password": "3864"
}

# ===== SBERT ëª¨ë¸ =====
MODEL_PATH = "jhgan/ko-sbert-sts"
model = SentenceTransformer(MODEL_PATH)


# ============================================
# ğŸ”¹ 1ï¸âƒ£ ê°•ì˜ì •ë³´ ì¹´í…Œê³ ë¦¬ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰
# ============================================
def _fetch_similar_chunks(query_embedding, top_k=1):
    """
    'ê°•ì˜ì •ë³´' ì¹´í…Œê³ ë¦¬ì—ì„œ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ìŒ
    """
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT 
            dc.doc_id,
            dc.chunk_id,
            dc.chunk_text,
            dc.category,
            1 - (e.embedding <#> %s::vector) AS similarity
        FROM embeddings e
        JOIN doc_chunks dc ON e.chunk_id = dc.chunk_id
        WHERE dc.category = 'ê°•ì˜ì •ë³´'
        ORDER BY e.embedding <#> %s::vector
        LIMIT %s;
        """,
        (query_embedding, query_embedding, top_k)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ 2ï¸âƒ£ ë™ì¼ ê³¼ëª©(doc_id)ì˜ ëª¨ë“  ì²­í¬ ê°€ì ¸ì˜¤ê¸° (ë‹¨, index=2 ì œì™¸)
# ============================================
def _fetch_all_chunks_by_doc(doc_id):
    """
    ê°™ì€ ê³¼ëª©(doc_id)ì˜ ëª¨ë“  ì²­í¬ë¥¼ chunk_index ìˆœìœ¼ë¡œ ë¶ˆëŸ¬ì˜´
    ë‹¨, chunk_index=2ì¸ ì²­í¬ëŠ” ì œì™¸
    """
    conn = psycopg2.connect(**PG_DSN)
    cur = conn.cursor()

    cur.execute(
        """
        SELECT chunk_index, chunk_text
        FROM doc_chunks
        WHERE doc_id = %s
          AND chunk_index != 2
        ORDER BY chunk_index ASC;
        """,
        (doc_id,)
    )

    rows = cur.fetchall()
    cur.close()
    conn.close()
    return rows


# ============================================
# ğŸ”¹ 3ï¸âƒ£ ì¶”ì²œ í•¨ìˆ˜ (í•œ ê³¼ëª© ë‹¨ìœ„)
# ============================================
def recommend_one_course(user_query):
    """
    'ê°•ì˜ì •ë³´' ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ 1ê°œ ê³¼ëª©(doc_id)ì„ ì°¾ì•„
    ëª¨ë“  ì²­í¬(ë‹¨, indexâ‰ 2)ë¥¼ ì¶œë ¥
    """
    # ì‚¬ìš©ì ë¬¸ì¥ ì„ë² ë”©
    query_embedding = model.encode(user_query).tolist()

    # ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ 1ê°œ
    rows = _fetch_similar_chunks(query_embedding, top_k=1)
    if not rows:
        return " ê´€ë ¨ ê°•ì˜ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # í•´ë‹¹ ì²­í¬ê°€ ì†í•œ ê°•ì˜ì˜ doc_id ì„ íƒ
    best_doc_id = rows[0][0]
    best_sim = round(float(rows[0][4]), 4)

    # ë™ì¼ ê°•ì¢Œì˜ ì „ì²´ ì²­í¬ ê°€ì ¸ì˜¤ê¸° (index 2 ì œì™¸)
    chunks = _fetch_all_chunks_by_doc(best_doc_id)
    if not chunks:
        return f" doc_id={best_doc_id} ì— ëŒ€í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # í…ìŠ¤íŠ¸ ê²°í•©
    full_text = "\n-----\n".join(
        [f"[{idx}] {txt}" for idx, txt in chunks]
    )

    # ê²°ê³¼ ë°˜í™˜
    return {
        "ì¶”ì²œ_ê°•ì˜_doc_id": best_doc_id,
        "ìœ ì‚¬ë„": best_sim,
        "í†µí•©_ê°•ì˜_ì •ë³´": full_text
    }


# ============================================
# ğŸ”¹ ì‹¤í–‰ ì˜ˆì‹œ
# ============================================
if __name__ == "__main__":
    query = "íšŒê³„ì‚¬ ë˜ê³ ì‹¶ì–´ìš”"
    print(f"\n[ì‚¬ìš©ì ì…ë ¥] {query}\n")

    rec = recommend_one_course(query)

    if isinstance(rec, str):
        print(rec)
    else:
        print(f" ì¶”ì²œ ê°•ì˜ (doc_id={rec['ì¶”ì²œ_ê°•ì˜_doc_id']}, ìœ ì‚¬ë„={rec['ìœ ì‚¬ë„']})\n")
        print(rec["í†µí•©_ê°•ì˜_ì •ë³´"])
